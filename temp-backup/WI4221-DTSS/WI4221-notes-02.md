---
documentclass: article
classoption: twocolumn
---
# Lecture 02: Stochastic Processes

## Concepts of Stochastic Processes
Stochastic processes are time-series of phenomena. A stochastic system is, fora signal with fluctuations, a more realistic and of lower complexity than a deterministic system.


\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Stochastic Processes)}.} \textit{Let $(\Omega, F), (X, G), T\subseteq \mathbb{Z}$. $x: \Omega \times T \to X$ is called a stochastic process if, $\forall t \in T,\; x(\cdot ,t): \Omega \to X$ is a random variable (a measureable function),
$$
\iff \forall t\in T, \forall A \in G, \underbrace{\{\omega \in \Omega| x(\omega, t) \in \}}_{x^{-1}(A)} \in F
$$
}}}
Notation commonly used
\begin{gather*}
x(t) = x_{t} = x_{t}(\omega) = x(\omega, t)\\
x = \{x(\omega, t)\in X, \forall t\in T, \forall \omega \in \Omega\}
\end{gather*}
Where $T$ is a discrete-time index set which can have a finite horizon, half-infinite forward horizon or infinite horizon. Commonly we call
$$
\forall \omega \in \Omega, x(\omega, \cdot ) : T \to X
$$
the **sample path** of the process.

## Families of Distributions
An important concept for stochastic processes is a family of finite-dimensional probability distributions. This is because every time step $t$ of the process corresponds to a distribution.

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Family of finite-dimensional pdfs)}.}\textit{
Let $x: \Omega \times T \to \R^{n}$, $T = \mathbb{N}$, then
}}}
<!-- $$ -->
<!-- P_{fdpdf} = -->
<!-- \left( -->
<!-- \begin{array} -->
<!--    \text{pdf}( \cdot ; x(t_{1}), \cdots , x(t_{m}))\|\\ -->
<!--    \forall m \in \mathbb{Z}_{+}, \forall t_{i}\in T, t_{i} < t_{j}, \forall i < j -->
<!-- \end{array} -->
<!-- \right\} -->
<!-- \right) -->
<!-- $$ -->
<!-- }}} -->

In principle we can approximate such distributions based on observations using statistical tools.


Kolmogorov (1950) formalized the notation of existence of stochastic processes. Kolmogorov's theorem proved for $T = [0, 1]$, then also true for $T = \R_{+}$ and for $T=\R$. Similarly true for $T = \mathbb{N}$

\fbox{\parbox{\columnwidth}{
\textbf{Theorem \textnormal{(Existence of Stochastic Process (Kolmogorov, 1950))}.}\textit{
}}}


Consider $T$ and $(X, G)$ two stochastic process on these spaces are considered _equivalent_ if their family of distributions is the same.

## Defining Stochastic Process


### Gaussian Process

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Gaussian Process)}.}\textit{
A stochastic process $x: \Omega \times T \to \R^{n}$ is called a Gaussian process is each member of its family of FDPDFs is a Gaussian pdf. In terms of notation we write
$$
\forall m \in \mathbb{Z}_{+}, \forall t_{i} \in T, t_{i} < t_{j} \text{ if } i < j, \{x(t_{1}), \cdots , x(t_{m})\} \in G
$$
}}}

Gaussian pdf is motivated by the central limit theorem, which states that scaled sum of a sequence of independent RVs converges to a Gaussian distribution.

### Bernoulli Process
Another common process is a Bernoulli process. This is used often in information theory and can model e.g. a stream of bits in a communication channel.

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Bernoulli Process)}.}\textit{
$x: \Omega \times T \to \{0, 1\}$, $\{x(0), x(1),\cdots \}$ is a sequence of i.i.d. random variables such that 
\begin{align*}
q(t) &=\\
1 - q(t) &=
\end{align*}
}}}

### Poisson Process


## Properties of Stochastic Process
We want to know if a process is integrable and square intebrable. It is is integrable if
\begin{gather*}
\forall t \in T, \forall i \in \mathbb{Z}_{n_{x}}, \mathbb{E}|x_{i}(t)| < \infty;\\
m_{x}(t) = \mathbb{E}\left[x(t)\right], m_{x}: T \to \R^{n_{x}}
\end{gather*}
Call $m_{x}$ the mean value function of $x$. Square integrable if

[insert definition square integrable]

A common idea that we need is positive-definite functions. A function $W: T \times T \to \mathbb{R}^{n_{x}\times n_{x}}$ is called a positive definite function if all entries of the matrix generated by the function are positive, that is
$$
0 \leq \sum_{i=1}^{m} \sum_{j=1}^{m} c_{i}^{T}W((t_{i}, t_{j}) x_{j}\footnote{Note that if the inequality here is strict, the it strictly positive definite}
$$
The function $W$ on $T = \mathbb{N}$ is a covariance function if and only if

1. $W(s, t) = W(t, s)^{T}\; \forall s,t \in T$ (closed w.r.t. transposition)
2. $W$ is positive-definite

Another important property is stationarity. A stochastic process is stationary if any FDPDF remains the same after a time-shift operation.
\begin{align*}
&x: \Omega \times T \to \R^{n_{x}}, T \subseteq \mathbb{Z}\\
&\text{if } \forall m\in\mathbb{Z}_{+}, \forall t_{i} \in T \text{ s.t. } i \in \mathbb{N}_{m},\\
&\forall s \in \mathbb{Z} \text{ s.t. } t_{i}+s \in T,\\
&p(x(t_{1}), \cdots , x(t_{m})) = p(x(t_{1}+s), \cdots , x(t_{m}+s))
\end{align*}

Note that often, we may need to remove a trend or something similar to "detrend" the data and transform it to a stationary process. Closely related to stationarity is time-invertibility. The definition is nearly the same, but instead we say reverse around a point in time $s$. Proving this is relatively straightforward. Because of this derivation, time-reversibility implies stationarity.
\begin{align*}
&x: \Omega \times T \to \R^{n_{x}}, T \subseteq \mathbb{Z}\\
&\text{if } \forall m\in\mathbb{Z}_{+}, \forall t_{i} \in T \text{ s.t. } i \in \mathbb{N}_{m},\\
&\forall s \in \mathbb{Z} \text{ s.t. } s-t_{i} \in T,\\
&p(x(t_{1}), \cdots , x(t_{m})) = p(s-x(t_{1}), \cdots , x(s-t_{m}))
\end{align*}

## Markov Processes
### Conditional Independence
A different perspective on independence is to formulate it in terms of expected values. It can be shown to be equivalent to the independence of $\sigma$-algebras definition.

Conditional indepedence is a generalisation of independence. Conditional independence is widely used used in Engineering and Mathematics.

\fbox{\parbox{\columnwidth}{
\textbf{Theorem \textnormal{(Conditional Independence Relation)}.}\textit{
Given probability space $(\Omega, F, P)$, $F_{1}, F_{2}, G \subseteq F$, sub-$\sigma$-algebras; we write
$$
\mathbb{E}\left[x_{1}x_{2}|G\right] = \mathbb{E}\left[x_{1}|G\right]\mathbb{E}\left[x_{2}|G\right]
$$
for all $x_{1}\in L(\Omega, F_{1}, \R_{+})$, $x_{2}\in L(\Omega, F_{2}, \R_{+})$. Notation $(F_{1}, F_{2}|G) \in CI$ where $CI$ denotes conditionally independent relation of sub-$\sigma$-algebras
}}}
As an elemantary property, $(F_{1}, F_{2} \vee G)$ independent $\implies$ $(F_{1}, F_{2}|G) \in CI$. The proof is relatively straightforward. 

For Gaussian RVs, we have 
$$
(y_{1}, y_{2}, x) \in G(0, Q_{(y_{1}, y_{2},x)})
$$
with $y_{1}: \Omega\to \R^{n_{y_{1}}}$, $y_{2}: \Omega\to \R^{n_{y_{2}}}$ and $x: \Omega\to\R^{n_{x}}$, $Q_{x} \succ 0$.

### Markov Processes Definition
A stochastic process is called a Markov Process if, for all times, the future and the past of the process are conditionally independent when conditioned on the present. Equivalently we write
$$
\forall t\in T, (F_{t}^{x+}, F_{t-1}^{x-1}|F^{x(t)})\in CI
$$
where $x: \Omega \times T \to X$, $(\Omega, F, P)$, $(X, G)$ and $F_{t}^{x+} = \sigma(\{x(s), \forall s \geq t\}$, $F_{t-1}^{x-} = \sigma(\{x(s), \forall s \leq t-1\}$. The future of the random process $x$ _only_ depends on the current state and not any past states. This holds for arbitrary non-linear transformations.

Interpretation of a Markov process in terms of measurable map from a state to a conditional measure on a future state
$$
x(s) \mapsto \text{cpdf}(x(t)| F_{s}^{x-})\; \forall s,t \in T, s < t
$$
where $\text{cpdf}$ denotes a _conditional probability distribution function_.

We can represent Markov processes recursively. If a Markov process is integrable, then there is a recursive representation of the form
\begin{align*}
&x: \Omega \times T \to \R^{n_{x}},\\
&x(t+1) = f(t, x(t)) + \Delta m(t), x(0) = x_{0},\\
&f(t, x(t)) = \mathbb{E}\left[x(t+1)|F^{x(t)}\right]\\
&m(t) = \sum_{s=1}^{t} \Delta m(s), m: \Omega \times T \to \R^{n_{x}}
\end{align*}
This process $\{m(t), F_{t}^{x}, t\in T\}$ is called a Martingale.


### Gaussian Processes
A process is Gaussian if every member of its family of FDPFs is Gaussian pdf. Gaussian processes are square-integrable and define
\begin{align*}
    m_{x}(t) &= \mathbb{E}\left[x(t)\right], m_{x}: T\to\R^{n_{x}}\\
    W_{x}(t, s) &= \mathbb{E}\left[(x(t) - m_{x}(t))(x(s)-m_{x}(s))^{T}\right],\\
    &W_{x}: T\times T\to\R^{n_{x}\times n_{x}}\\
    Q_{x}(t) &= \mathbb{E}\left[(x(t) - m_{x}(t))(\cdot )^{T}\right],\\
    &Q_{x}: T\to\R_{pds}^{n_{x}\times n_{x}}
\end{align*}
Note that $Q_{x}(t) = W_{x}(t,t)$ for all $t\in T$.

For stationary Gaussian processes, we have $W_{x}(t) = W_{x}(t, 0) = W_{x}(t+r, r)$ for all $r \in \mathbb{Z}$. 

Some common types of Gaussian processes are

- Gaussian White Noise, $v(t) \in G(m_{v}(t), Q_{v}(t))$
- Stationary Gaussian White Noise, $v(t) \in G(m_{v}, Q_{v})$
- Standard Stationary Gaussian White Noise, $v(t) \in G(0, I_{n_{v}})$


\fbox{\parbox{\columnwidth}{
\textbf{Theorem \textnormal{(Representation of a Gauss-Markov Process)}.}\textit{
Consider a Gaussian process with the notation $x: \Omega \times T \to \R^{n_{x}}$, $x(t) \in G(0, Q_{x}(t))$. We assume that for all $t\in T$, $Q_{x}(t) \succ 0$. If $x$ has the representation
\begin{align*}
    &x(t+1) = A(t)x(t) + M(t)v(t), x(0)=x_{0}\\
    &x_{0}: \Omega \to \R^{n_{x}}, x_{0} \in G(0, Q_{x_{0}}), Q_{x_{0}} \succ 0\\
    &v: \Omega \to \R^{n_{v}}, \text{ Standard Gaussian White Noise}\\
    &F^{x_{0}}, F^{v}_{\infty} \text{ are independent}\\
    &A: T \to \R^{n_{x}\times n_{x}}, M: T\to\R^{n_{x}\to n_{v}}
\end{align*}
then it is a Gauss-Markov process.
}}}

A natural question to ask is, given a Gaussian process, when is it Markov? We can derive that the Gaussian process if Markov if we can factorize the covariance matrix as
$$
W_{x}(t, s) = W_{x}(t, r)W_{x}(r,r)^{-1}W_{x}(r,s)
$$
where $s < r < t$ and $s,r,t \in T$.

## Finite-Valued Processes
Consider 
$$
x: \Omega \times T \to \mathbb{Z}_{n_{i_{x}}} = \{1, 2, \cdots , n_{i_{x}}\} \subset \mathbb{Z}
$$
We define an indicator process of the finite valued process $x$ according to 
$$
i_{x,j}(\omega, t) = \begin{cases}
1, \text{ if } x(\omega, t) = j\\
0, \text{ otherwise}
\end{cases}
$$
Now we can define a vector of indicator variables and represent 
$$
x(t) = C_{x}i_{x}(t)
$$
We again find that the representation of a Finite-valued Markov process is represented in the form
\begin{align*}
    &i_{x}(t+1) = Ai_{x}(t) + \Delta m(t), i_{x}(0) = i_{x,0}\\
    &x(t) = C_{x}i_{x}(t)\\
    &\text{with $A \in \R^{n_{i_{x}} \times n_{i_{x}}}_{st,+}$ a stochastic matrix}\\
    &Ai_{x}(t) = \mathbb{E}\left[i_{x}(t+1)|F^{x(t)}\right] = \mathbb{E}\left[i_{x}(t+1) | F^{i_{x}(t)}\right]\\
    &0 = \mathbb{E}\left[\Delta m(t) | F^{x}_{t}\right]\; \forall t \in T\\
    &\Delta m: \Omega \times T \to \R^{n_{i_{x}}}
\end{align*}
We can prove that $\Delta m(t)$ is what is called a martingale increment at time $t\in T$.
