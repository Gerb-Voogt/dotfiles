---
documentclass: article
classoption: twocolumn
---

# Lecture 02: Markov Chains

## Stochastic Processes and Markov Chains

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Discrete-Time Stochastic Process)}.}\textit{
Informally, a stochastic process is a family of random variables $\{X_{t}\}_{t\in N_{0}}$ such that $X_{t}: \mathbb{X} \times T \to X$.\\
\\
More formally, let $(\mathbb{X}, \mathcal{F}, \mathbb{P})$ be a probability space with filtration (family of $\sigma$-algebras) $\{\mathcal{F}_{t}\;|\; t \in T\}$, $T$ and index set and $(X, \mathcal{G})$ be a measurable space (commonly $\mathcal{G} = \mathcal{B}(X)$). A stochastic process on these spaces is a function $x: \mathbb{X} \times T \to X$ such that for all $t$, $x(\cdot , t): \mathbb{X} \to X$ is measurable, or in other words a random variable.
}}}

An important sub-class of stochastic processes is Markov processes. Markov processes are defined by the property that the past and the future of a process are conditionally independent given the current state. That is:
$$
(\mathcal{F}^{x_{+}}_{t}, \mathcal{F}^{x_{-}}_{t-1}\;|\: \mathcal{F}^{x(t)}) \in CI
$$
Where $\mathcal{F}^{x_{-}}_{t-1}$ are all the past $\sigma$-algebras denoted $\sigma(\{ x(s)\; | s \leq t-1)\})$ and $\mathcal{F}^{x_{+}}_{t}$ are all the future $\sigma$-algebras.
Informally, we say that given the current state, we gain no extra information by knowing either the past or the future of the process.

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Markov Process)}.}\textit{
A discrete-time stochastic process $\{X_{t}\}_{t\in N_{0}}$ is a Markov Chain if, for all $t$
$$
\mathbb{P}\left(X_{t+1}\in A\;|\; \mathcal{F}^{x}_{t}\right) = \mathbb{P}\left(X_{t+1}\in A\;|\; \mathcal{F}^{x(t)}\right)
$$
Where $A\subset \mathbb{X}$. Alternatively, one can write
$$
\left(\mathcal{F}^{x+}_{t}, \mathcal{F}^{x-}_{t-1}\; |\; \mathcal{F}^{x(t)}\right)\in CI
$$
That is, the current state makes the past and future of the process conditionally independent.
}}}


\fbox{\parbox{\columnwidth}{
\textbf{Example \textnormal{(Stock Prices)}.}
The most basic example of a Markov Chain are closing prices of a stock on day $t\in N$ with $X_{0} = 100$. We define $R_{t+1}\sim \mathcal{N}(0, 0.01)$ as the rate of reutrn of the stock from day $t$ to day $t+1$. We assume the rate of return to be an i.i.d. sequence. We have
$$
X_{t+1} = X_{t}(1 + R_{t+1}) \quad t \in N_{0}
$$
\begin{center}
    \includegraphics[width=0.5\columnwidth]{images/02-stocks-example.png}\\
\textit{\textbf{Fig.} An example of multiple realisation of the same process starting at $t = 0$ and $X_{0} = 100$}
\end{center}
}}\newline\newline
We will now show a case where the process is **NOT** Markov. Consider now the case that 
$$
R_{t+1}\sim \mathcal{N}\left( \frac{1}{R_{t} + 1}, 1\right)
$$
Note that
\begin{align*}
   X_{t+1}  &= X_{t}(1 + R_{t+1})\\
   X_{t}  &= X_{t-1}(1 + R_{t})
\end{align*}
But since it follows that $R_{t}$ distribution is dependent on $X_{t-1}$ we have that this rule has $X_{t+1}$ depend on both $X_{t}$ as well as $X_{t-1}$, implying that the process is not Markov.


\fbox{\parbox{\columnwidth}{
\textbf{Example \textnormal{(Coin Flipping)}.}
Let $X_{t}$ be the number of heads observed in $t\in N$ flips with $X_{0} = 0$. We now define $W_{t}$ as a Bernoulli process with $p = \frac{1}{2}$ as each time step represents a 50/50 chance of flipping heads. We write that 
$$
X_{t} = \sum_{i=1}^{t} W_{i} = X_{t-1} + W_{t}
$$
\begin{center}
    \includegraphics[width=0.5\columnwidth]{images/02-coins-example.png}\\
\textit{\textbf{Fig.} An example of a Bernoulli process of coin flipping modelled as a Markov Chain (Number of heads at time-step $t+1$ only depends on the number of heads at time-step $t$ and an independent random noise process).}
\end{center}
}}

Note that the recursion relation for a Markov chain may in general be non-linear. That is
$$
X_{t+1} = f(X_{t}, W_{t+1}), \quad t \in N_{0}
$$
Which describes a Markov Chain if the disturbance $W_{t+1}$ is conditionally independent given $X_{t}$. Note that in many cases, we can convert non-Markovian stochastic processes into Markov chains by using state augmentation:
$$
Z_{t+1} = f(Z_{t}, Z_{t-1}, \cdots , Z_{t-m}) \implies X_{t+1} = \tilde f(X_{t})
$$
With $X_{t} = [Z_{t}, Z_{t-1}, \cdots , Z_{t-m}]^{T}$ and where the process $X_{t}$ is a Markov Chain. Note that in general this leads to state-space explosion (i.e. very high-dimensional models). So the key to good Markov Chain models is to control the size of this state space and ensure tractable models.


## Countable State Markov Chains
We will now only consider Markov chains with countable state space. That is $\mathbb{X} = [n] = \{1, 2,, \cdots , n\}$ with $n \in \mathbb{N}$. Note that strictly speaking we do not need to adhere to enumerating the states in this way, they can be arbitrary but need to admit a bijection to the set of natural numbers. For these types of processes (i.e. with an enumerable state space) we can describe state transition in terms of a state transition matrix.

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(State Transition Matrix)}.}\textit{
Let $P_{t}\in [0, 1]^{n\times n}$ be the collection of probability of transitioning from state $i$ to state $j$ at time $t+1$. We have
$$
P_{t}(i, j) = \mathbb{P}(X_{t+1} = j\;|X_{t} = i) \quad i, j \in \mathbb{X} = [n]
$$
}}}

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Time-Homogeneous Markov Chain)}.}\textit{
A Markov Chain with time-independent transition probability matrices $P_{t} = P \in [0, 1]^{n\times n}$ for all $t \in N_{0}$ is called time-homogeneous. We have 
$$
P(i, j) = \mathbb{P}(X_{t+1} = j\; |\; X_{t} = i) \quad \forall t \in N_0
$$
$P$ is referred to as a matrix even in the case that $|\mathbb{X}|$ is countably infinite even though technically, in this case it is a linear operator.
}}}

Countable state Markov Chains can graphically be represented in terms of graphs. This representation provides the exact same information as the probability transition matrix $P$, however it might be more interpretable and better highlight symmetries.


\begin{figure}[h]
    \center
    \begin{tikzpicture}
        \node[draw, circle, minimum width=1cm] (left) {$i$};
        \node[draw, circle, right of = left, node distance = 2cm, minimum width = 1cm] (right) {$j$};
        \draw[->, bend left, line width=1pt] (left) to [bend left]node[above]{$P(i,j)$} (right);
        \draw[->, bend left, line width=1pt] (right) to [bend left]node[below]{$P(j,i)$} (left);
    \end{tikzpicture}
    \caption{Simplified Graphical Representation of a Markov Chain.}
\end{figure}

\fbox{\parbox{\columnwidth}{
\textbf{Example \textnormal{(Coin Tossing Markov Chain Model)}.}
Consider again the coin flipping process where we have $X_{t}$ model the amount of heads after $t$ tosses. We model the process as
$$
X_{t+1} = X_{t} + W_{t}
$$
Where the amount of heads $X$ at time-step $t+1$ is the previous amount of heads with added Bernoulli noise ($p = 0.5$). For the state transition matrix we write
\tiny
$$
P = \begin{bmatrix}
    1/2 & 1/2 & 0 & \cdots \\
    0 & 1/2 & 1/2 & \cdots \\
    0 & 0 & 1/2 & \cdots \\
    \vdots & \vdots & \ddots & \ddots
\end{bmatrix}
$$\normalsize
\begin{center}
\begin{tikzpicture}[scale=0.7, transform shape]
        \node[draw, circle, minimum width=1cm] (first) {$0$};
        \node[draw, circle, right of = first, node distance = 2cm, minimum width = 1cm] (second) {$1$};
        \node[draw, circle, right of = second, node distance = 2cm, minimum width = 1cm] (third) {$2$};
        \node[right of=third] (etc) {$\cdots$};
        % Drawing lines
        \draw[->, bend left, line width=1pt] (first) to [bend left]node[above]{$\frac{1}{2}$} (second);
        \draw[->, bend left, line width=1pt] (second) to [bend left]node[above]{$\frac{1}{2}$} (third);
        \draw (first) edge[loop below, line width=1pt] node {$\frac{1}{2}$} (first);
        \draw (second) edge[loop below, line width=1pt] node {$\frac{1}{2}$} (second);
        \draw (third) edge[loop below, line width=1pt] node {$\frac{1}{2}$} (third);
    \end{tikzpicture}\\
    \textit{\textbf{Fig.}Markov Chain for the coin flipping example}
\end{center}
}}


\fbox{\parbox{\columnwidth}{
\textbf{Theorem \textnormal{(Stochastic Matrices)}.}\textit{
The transition matirx $P$ of a Markov Chain is a (row) stochastic matrix satisfying
\begin{enumerate}
    \item $P(i, j) \geq 0$ for all $i, j \in \mathbb{X}= [n]$
    \item $\sum_{j\in\mathbb{X}} P(i, j) 1$ for all $i \in \mathbb{X}= [n]$
\end{enumerate}
}}}

\fbox{\parbox{\columnwidth}{
\textbf{Lemma \textnormal{(Stochastic Matrix Properties)}.}\textit{
If $P$ is a stochastic matrix, then the following properties apply
\begin{enumerate}
    \item The vector $\boldsymbol{1}_{n}$ is an eigenvector of $P$
    \item $\text{spec}(P) \subset \mathbb{D}_{c}$ where $\mathbb{D}_{c} = \{z \in \mathbb{C}\;|\; |z| \leq 1\}$.
    \item $P^{s}$ is a stochastic matrix for all $s \in \mathbb{N}$
\end{enumerate}
}}}

\fbox{\parbox{\columnwidth}{
\textbf{Lemma \textnormal{(Mutli-step transition probability)}.}\textit{
For each $t\in N_{0}$, we have that the state transition probability for multiple steps can be given as
$$
\mathbb{P}(X_{t+s}= j\;|\; X_{t}= i) = P^{s}(i, j)\quad \forall i, j \in \mathbb{X},\; s\in\mathbb{N}
$$
}}}


\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Countable State Markov Chain)}.}\textit{
The tuple $(\mathbb{X}, P, p_{0})$ describing stochastic process $X: \mathbb{X} \times \Omega \to X$ with countable state space $\mathbb{X} = [n]$, transition probability matrix $P\in\R^{n\times n}$ and initial distribution $p_{0} \in \Delta(\mathbb{X})$ ($p_{0}(i) = \mathbb{P}(X_{0} = i)$ for all $i \in \mathbb{X}$ is a \textbf{countable-state Markov Chain}.
}}}
Note: the notation $\Delta(\cdot)$ denotes the probability simplex over a set, so the set $\Delta(\mathbb{X})$ denotes the set of all probability distributions over $\mathbb{X}$.

\fbox{\parbox{\columnwidth}{
\textbf{Lemma \textnormal{(Trajectories and State distributions)}.}\textit{
Consider a countable-state Markov Chain $(\mathbb{X}, P, p_{0})$. For each trajectory $\{i_{j}\}_{j=1}^{n} \in \mathbb{X}^{t+1}$ we have that 
$$
\mathbb{P}(X_{t} = i_{t}, \cdots , X_{0}= i_{0}) = p_{0}(i_{0})P(i_{0}, i_{1}) \cdots P(i_{t-1}, i_{t})
$$
Now let $p_{t}\in \Delta(\mathbb{X})$ be the state distribution at time $t$. That is, $p_{t} = \mathbb{P}(X_{t} = i)$ for all $i \in \mathbb{X}$. Fir each $t\in N_{0}$ we have 
$$
p_{t} = p_{0}P^{t}
$$
Where $p_{\cdot} \in \R^{1\times n}$ (treated as a row vector).
}}}


\fbox{\parbox{\columnwidth}{
\textbf{Example \textnormal{(Coin Flips Distribution)}.}
Consider the initial conditions $X_{0} = 0$, $X_{t} \in \mathbb{X} = \{0\} \cup [n]$. Where $X_{t}$ models the observed number of heads in $t$ flips. We start again by recognizing that $P(i, i) = \frac{1}{2}$ and $P(i, i+1) = \frac{1}{2}$. We start with initial distribution $p_{0} = [1,\; 0,\; 0,\;\cdots]$. We can derive a recurrence relation for the state as
$$
\mathbb{P}(X_{t+1} = x) = \frac{1}{2}\mathbb{P}(X_{t} = x) + \frac{1}{2}\mathbb{P}(X_{t} = x-1)
$$
This is the recurrence relation for a binomial distribution as $X_{t} = x$ in $t$ steps can happen in $\binom{t}{x}$ ways. The resulting distribution is hence
$$
\mathbb{P}(X_{t} = x) = \binom{t}{x}\left(\frac{1}{2}\right)^{t}
$$
}}

## Limiting Distributions
We again only consider time-homogeneous finite-state Markov Chains. We are now interested in the long term behaviour of the Markov Chain. To do we analyse the asymptotic distribution of the state. That is, $p_{t}\in \Delta(\mathbb{X})$ as $t\to \infty$.

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Limiting Distributions)}.}\textit{
Given Markov Chain $(\mathbb{X}, P, p_{0})$ with finite sample space $\mathbb{X} = [n]$ The Limiting distribution is given as $p_{\infty} = \lim_{t\to \infty} p_{t}$ for any $p_{0} \in \Delta(\mathbb{X})$.
}}}

The definition above leads to 2 interesting questions:

1. Does $p_{\infty}$ always exist and is it unique and independent of initial conditions?
2. How can we compute $p_{\infty}$ if it exists and is unique?

\fbox{\parbox{\columnwidth}{
\textbf{Lemma \textnormal{(Limiting Distributions - I)}.}\textit{
By the lemma on state distributions we have $p_{t} = p_{0}P^{t}$. For the limiting distribution we write
$$
p_{\infty} = \lim_{t\to \infty} p_{t} \quad \forall p_{0} \in \Delta(\mathbb{X}) \iff \lim_{t\to \infty} P^{t} = \boldsymbol{1}_{n} p_{\infty}
$$
Where $\boldsymbol{1}_{n} = [1, 1, \cdots, 1]^{T}$.
}}}

To easily compute $P^{t}$, we can leverage the digitalisation of $P$, which exists based on the fact that $P$ is a stochastic matrix. We write
$$
P^{t} = T \Lambda^{t} T^{-1} = \sum_{i=1}^{n} \lambda_{i}^{t}\underbrace{(r_{i} \cdot q_{i})}_{:= A_{i}}
$$
Where $r_{i}$ and $q_{i}$ follow from
$$
T = \begin{bmatrix}
    r_{1} & \cdots & r_{n}
\end{bmatrix}, \quad
T^{-1} = \begin{bmatrix}
    q_{1}\\
    \vdots\\
    q_{n}
\end{bmatrix}
$$
and hence $r_{i} \in \R^{n}$, $q_{i}\in \R^{1\times n}$ and $(r_{i} \cdot q_{i})\in \R^{n\times n}$. As a remark on this, $T$ and $T^{-1}$ are not always easy to compute, but we can form a system of $n^{3}$ linear equations for the $n^{3}$ entries of $\{A_{i} \in \R^{n\times n}\}_{i=1}^{n}$ by using 
$$
P^{t} = \sum_{i=1}^{n} A_{i}\lambda_{i}^{t}
$$

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Invariant Distributions)}.}\textit{
A distribution $\pi$ is considered invariant if $\pi = \pi P \in \Delta(\mathbb{X})$. Furthermore, if $p_{t_{0}} = \pi$ for some $t_{0} \in \mathbb{N}_{0}$, then $p_{t} = \pi$ for all $t$. The invariant distribution always exists but it may not be unique.
}}}

\fbox{\parbox{\columnwidth}{
\textbf{Lemma \textnormal{(Limiting Distributions - II)}.}\textit{
The limiting distribution is an invariant distribution.
}}}

As a remark the lemma above: **Not all invariant distribution are necessarily limiting distributions.**.

\fbox{\parbox{\columnwidth}{
\textbf{Lemma \textnormal{(Invariant and Limiting Distributions of a finite state process)}.}\textit{
Consider finite state Markov Chain $(\mathbb{X}, P, p_{0})$ with $\mathbb{X}= [n]$. Let $\lambda_{i} \in \text{spec}(P)$ such that $1 = \lambda_{1} \geq |\lambda_{2}| \geq \cdots \geq |\lambda_{n}|$. Let $\pi = \pi P\in \Delta(\mathbb{X})$ denote an invariant distribution of the Markov Chain. Then:
\begin{itemize}
   \item $\pi$ is a unique invariant distribution if and only $\lambda_{i} \neq 1$ for all $i\neq 1$.
   \item $\pi$ is the limiting distribution of the Markov Chain if and only if $|\lambda_{i}|<1$ for all $i\neq 1$, in which case, $\exists m\in [n-1]$ and $C>0$ such that 
    $$
    \lim_{t\to \infty}|P^{t}(i, j) - \pi(j)|\leq C t^{m}\cdot |\lambda_{2}|^t,\quad \forall i, j \in \mathbb{X}
    $$
\end{itemize}
}}}

## Markov Reward Processes
Markov Reward Processes are an extension of Markov Chains with a reward function $r: \mathbb{X} \to \R$ associating scalar values to different states.  These types of Markov chains are very important for optimal control as well as sequential decision making and reinforcement learning.

An example application is a queuing model. We associate the reward $1$ with states $x>0$ and $0$ to the state $x=0$ to signal whether the server is busy or not.

\fbox{\parbox{\columnwidth}{
\textbf{Definition \textnormal{(Markov Reward Process)}.}\textit{
A Markov reward process is characterized by the tuple $(\mathbb{X}, P, r, p_{0})$ where $\mathbb{X}$ is a sample space, $P$ a probability transition matrix, $r: \mathbb{X}\to \R$ a reward function and $p_{0}$ an initial state distribution.
}}}

\fbox{\parbox{\columnwidth}{
\textbf{Lemma \textnormal{(Expected Reward)}.}\textit{
For a finite-state Markov Reward Process, we have 
\begin{itemize}
    \item The expected reward at time $t$ is $\mathbb{E}\left[r(X_{t})\right] = p_{0}P^{t}r$
    \item Assuming that the limiting distribution exists, the limiting expected reward is $\lim_{t\to \infty} \mathbb{E}\left[r(X_{t})\right] = p_{\infty} \cdot r$
\end{itemize}
}}}

